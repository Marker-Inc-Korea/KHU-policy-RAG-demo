# In this yaml, we do not use tree_summarize for accuracy
# And did not use monoT5, because it can take too long.
node_lines:
- node_line_name: retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: retrieval
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
      top_k: 3
      modules:
        - module_type: bm25
        - module_type: vectordb
          embedding_model: openai
        - module_type: hybrid_rrf
          target_modules: ('bm25', 'vectordb')
          rrf_k: [3, 5, 10]
        - module_type: hybrid_cc
          target_modules: ('bm25', 'vectordb')
          weights:
            - (0.5, 0.5)
            - (0.3, 0.7)
            - (0.7, 0.3)
    - node_type: passage_reranker
      strategy:
        metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
      top_k: 1
      modules:
        - module_type: upr
- node_line_name: post_retrieve_node_line  # Arbitrary node line name
  nodes:
    - node_type: prompt_maker
      strategy:
        metrics: [bleu, meteor, rouge]
        generator_modules:
          - module_type: llama_index_llm
            llm: openai
            batch: 2
      modules:
        - module_type: fstring
          prompt:
            - "주어진 passage만을 이용하여 question에 따라 답하시오 passage: {retrieved_contents} \n\n Question: {query} \n\n Answer:"
            - "passage에는 question에 대한 답이 있습니다. 다음 question에 대답하시오. \n\n Passage: {retrieved_contents} \n\n Question: {query} \n\n 질문에 답하시오. 단계별로 생각하시오." # Zero-shot CoT prompt
    - node_type: generator
      strategy:
        - metric_name: bleu
        - metric_name: meteor
        - metric_name: sem_score
          embedding_model: openai
      modules:
        - module_type: llama_index_llm
          llm: openai
          temperature: [0.1, 0.5, 1.1]
          batch: 2
